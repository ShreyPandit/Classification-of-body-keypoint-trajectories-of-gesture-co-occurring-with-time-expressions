# Classification-of-body-keypoint-trajectories-of-gesture-co-occurring-with-time-expressions

## About the Project
The way humans interact with each other occurs in multimodality. We not only articulate words, but also we show them. Expressing different concepts such as time, place, and emotion comes with speech and some movements called body gestures. In this study, we propose a multi-modal method that captures different patterns of body gestures aligned with an articulated time expression such as “from beginning to end”. Our proposed architecture lies on two neural networks, the Compact Transformer and the Long-Short Term Memory (LSTM). Compact Transformer, the current State-of-the-art structure for temporally distributed images, also performs well over low-resourced data. LSTM performs well over temporal data with long-term dependencies. The hypothesis of this project is the existence of a relation between the body gestures and the time expressions we speak, and using neural networks, and we try to accept or reject this hypothesis.

## Blogs explaining the code and formulation of code in detailed steps - 
Weekly Tracker - [Blog](https://shreypandit.medium.com/gsoc-2022-red-hen-lab-f01b40d99f4c) <br>
Explanation of project - [Blog](https://shreypandit.medium.com/classification-of-body-keypoint-trajectories-of-gesture-co-occurring-with-time-expressions-4bcd9d9d2541) <br>
Interactable code on Colab for easier replication - [Colab Notebook](https://colab.research.google.com/drive/1iPSIDsazw0e52MIwq_f3bJKx-V4WGp7K?usp=sharing) <br>
Link to the proposal [Colab Notebook](https://sites.google.com/site/distributedlittleredhen/summer-of-code/red-hen-lab-gsoc-2022-projects) <br>
